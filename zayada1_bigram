{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7769268,"sourceType":"datasetVersion","datasetId":4544962}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn.functional as F\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-08T19:35:20.743183Z","iopub.execute_input":"2024-03-08T19:35:20.743741Z","iopub.status.idle":"2024-03-08T19:35:20.761836Z","shell.execute_reply.started":"2024-03-08T19:35:20.743700Z","shell.execute_reply":"2024-03-08T19:35:20.760512Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/kaggle/input/arabic-names/all_arabic_names.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"words = open('../input/arabic-names/all_arabic_names.txt', 'r').read().splitlines()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:35:20.763841Z","iopub.execute_input":"2024-03-08T19:35:20.764279Z","iopub.status.idle":"2024-03-08T19:35:20.772629Z","shell.execute_reply.started":"2024-03-08T19:35:20.764246Z","shell.execute_reply":"2024-03-08T19:35:20.771409Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Tokenize characters by assigning them numerical value\n# '.' is special value to indicate beginning/end of name\n\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:35:20.774327Z","iopub.execute_input":"2024-03-08T19:35:20.775132Z","iopub.status.idle":"2024-03-08T19:35:20.788551Z","shell.execute_reply.started":"2024-03-08T19:35:20.775089Z","shell.execute_reply":"2024-03-08T19:35:20.787246Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create data and label for neural net\n# Data is first character, label is the direct next one\n\nxs = []\nys = []\nb = {}\n\nfor word in words:\n    chs = ['.'] + list(word) + ['.']\n    for c1, c2 in zip(chs, chs[1:]):\n        ix1 = stoi[c1]\n        ix2 = stoi[c2]\n        \n        xs.append(ix1)\n        ys.append(ix2)\n        \nxs = torch.tensor(xs)\nys= torch.tensor(ys)\nnum = xs.nelement()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:35:20.790720Z","iopub.execute_input":"2024-03-08T19:35:20.791118Z","iopub.status.idle":"2024-03-08T19:35:20.874830Z","shell.execute_reply.started":"2024-03-08T19:35:20.791088Z","shell.execute_reply":"2024-03-08T19:35:20.873532Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# encode data with one-hot\n\nX = F.one_hot(xs, 48).float()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:35:20.876308Z","iopub.execute_input":"2024-03-08T19:35:20.876787Z","iopub.status.idle":"2024-03-08T19:35:20.886944Z","shell.execute_reply.started":"2024-03-08T19:35:20.876750Z","shell.execute_reply":"2024-03-08T19:35:20.885752Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# randomly initialise weights\n\nW = torch.randn((48, 48), requires_grad=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:35:20.888602Z","iopub.execute_input":"2024-03-08T19:35:20.888970Z","iopub.status.idle":"2024-03-08T19:35:20.897101Z","shell.execute_reply.started":"2024-03-08T19:35:20.888941Z","shell.execute_reply":"2024-03-08T19:35:20.895788Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Train network\n\nfor i in range(500):\n#     Forward pass\n    logits = X @ W\n    exp = logits.exp()\n    probs = exp/exp.sum(1, keepdims=True)\n    \n    nll = - probs[torch.arange(num), ys].log().mean() # loss\n    nll += 0.01 * (W**2).mean() # regularisation\n    \n    if (i % 10 == 0):\n        print(f'loss after {i} iterations: {nll}')\n        \n#     Backward pass\n    W.grad = None\n    nll.backward()\n    W.data += - 100 * W.grad\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:35:20.899023Z","iopub.execute_input":"2024-03-08T19:35:20.899815Z","iopub.status.idle":"2024-03-08T19:35:38.083083Z","shell.execute_reply.started":"2024-03-08T19:35:20.899776Z","shell.execute_reply":"2024-03-08T19:35:38.082136Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"loss after 0 iterations: 4.498400688171387\nloss after 10 iterations: 2.9593849182128906\nloss after 20 iterations: 2.844825029373169\nloss after 30 iterations: 2.800931453704834\nloss after 40 iterations: 2.7779858112335205\nloss after 50 iterations: 2.763876438140869\nloss after 60 iterations: 2.7542572021484375\nloss after 70 iterations: 2.7472379207611084\nloss after 80 iterations: 2.741877794265747\nloss after 90 iterations: 2.7376556396484375\nloss after 100 iterations: 2.73425555229187\nloss after 110 iterations: 2.731470823287964\nloss after 120 iterations: 2.7291600704193115\nloss after 130 iterations: 2.7272210121154785\nloss after 140 iterations: 2.7255771160125732\nloss after 150 iterations: 2.724170207977295\nloss after 160 iterations: 2.7229552268981934\nloss after 170 iterations: 2.721897602081299\nloss after 180 iterations: 2.7209694385528564\nloss after 190 iterations: 2.720149040222168\nloss after 200 iterations: 2.7194199562072754\nloss after 210 iterations: 2.718768358230591\nloss after 220 iterations: 2.7181825637817383\nloss after 230 iterations: 2.717654228210449\nloss after 240 iterations: 2.7171754837036133\nloss after 250 iterations: 2.7167396545410156\nloss after 260 iterations: 2.716341972351074\nloss after 270 iterations: 2.715977668762207\nloss after 280 iterations: 2.7156436443328857\nloss after 290 iterations: 2.7153356075286865\nloss after 300 iterations: 2.7150516510009766\nloss after 310 iterations: 2.7147884368896484\nloss after 320 iterations: 2.7145445346832275\nloss after 330 iterations: 2.714317560195923\nloss after 340 iterations: 2.714106798171997\nloss after 350 iterations: 2.7139101028442383\nloss after 360 iterations: 2.713726043701172\nloss after 370 iterations: 2.7135536670684814\nloss after 380 iterations: 2.713392496109009\nloss after 390 iterations: 2.7132408618927\nloss after 400 iterations: 2.7130987644195557\nloss after 410 iterations: 2.7129645347595215\nloss after 420 iterations: 2.7128381729125977\nloss after 430 iterations: 2.712719202041626\nloss after 440 iterations: 2.712606191635132\nloss after 450 iterations: 2.7124996185302734\nloss after 460 iterations: 2.7123987674713135\nloss after 470 iterations: 2.712303400039673\nloss after 480 iterations: 2.7122128009796143\nloss after 490 iterations: 2.7121267318725586\n","output_type":"stream"}]},{"cell_type":"code","source":"# finally, sample from the 'neural net' model\n\nfor i in range(10):\n  \n  out = []\n  ix = 0\n  while True:\n    \n    # ----------\n    # BEFORE:\n    #p = P[ix]\n    # ----------\n    # NOW:\n    X = F.one_hot(torch.tensor([ix]), num_classes=48).float()\n    logits = X @ W # predict log-counts\n    counts = logits.exp() # counts, equivalent to N\n    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n    # ----------\n    \n    ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:35:38.084706Z","iopub.execute_input":"2024-03-08T19:35:38.085153Z","iopub.status.idle":"2024-03-08T19:35:38.102843Z","shell.execute_reply.started":"2024-03-08T19:35:38.085115Z","shell.execute_reply":"2024-03-08T19:35:38.101348Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"اظ.\nأب.\nر.\nمعفاباي.\nيلره.\nنادانورضالمايت.\nعلح.\nعون.\nسعالار.\nمة.\n","output_type":"stream"}]}]}